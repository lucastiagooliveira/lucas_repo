{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark SQL Query DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\lucas\\Documents\\lucastiagooliveira\\Study Materials\\Ex_Files_Spark_SQL_DataFrames\\Exercise Files\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path + \"/utilization.csv\"\n",
    "df = spark.read.format(\"csv\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+----+---+\n",
      "|                _c0|_c1| _c2| _c3|_c4|\n",
      "+-------------------+---+----+----+---+\n",
      "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
      "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
      "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
      "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
      "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
      "|03/05/2019 08:31:14|100|0.41|0.58| 48|\n",
      "|03/05/2019 08:36:14|100|0.57|0.35| 58|\n",
      "|03/05/2019 08:41:14|100|0.41|0.40| 58|\n",
      "|03/05/2019 08:46:14|100|0.53|0.35| 62|\n",
      "|03/05/2019 08:51:14|100|0.51|0.60| 45|\n",
      "+-------------------+---+----+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_c0\",\"event_datatime\")\\\n",
    "         .withColumnRenamed(\"_c1\",\"server_\")\\\n",
    "         .withColumnRenamed(\"_c2\",\"cpu_utilization\")\\\n",
    "         .withColumnRenamed(\"_c3\",\"server_id\")\\\n",
    "         .withColumnRenamed(\"_c4\",\"session_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+---------+-------------+\n",
      "|     event_datatime|server_|cpu_utilization|server_id|session_count|\n",
      "+-------------------+-------+---------------+---------+-------------+\n",
      "|03/05/2019 08:06:14|    100|           0.57|     0.51|           47|\n",
      "|03/05/2019 08:11:14|    100|           0.47|     0.62|           43|\n",
      "|03/05/2019 08:16:14|    100|           0.56|     0.57|           62|\n",
      "|03/05/2019 08:21:14|    100|           0.57|     0.56|           50|\n",
      "|03/05/2019 08:26:14|    100|           0.35|     0.46|           43|\n",
      "|03/05/2019 08:31:14|    100|           0.41|     0.58|           48|\n",
      "|03/05/2019 08:36:14|    100|           0.57|     0.35|           58|\n",
      "|03/05/2019 08:41:14|    100|           0.41|     0.40|           58|\n",
      "|03/05/2019 08:46:14|    100|           0.53|     0.35|           62|\n",
      "|03/05/2019 08:51:14|    100|           0.51|     0.60|           45|\n",
      "+-------------------+-------+---------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|     0.51|           47|\n",
      "|     0.62|           43|\n",
      "|     0.57|           62|\n",
      "|     0.56|           50|\n",
      "|     0.46|           43|\n",
      "|     0.58|           48|\n",
      "|     0.35|           58|\n",
      "|     0.40|           58|\n",
      "|     0.35|           62|\n",
      "|     0.60|           45|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| sid| sc|\n",
      "+----+---+\n",
      "|0.51| 47|\n",
      "|0.62| 43|\n",
      "|0.57| 62|\n",
      "|0.56| 50|\n",
      "|0.46| 43|\n",
      "|0.58| 48|\n",
      "|0.35| 58|\n",
      "|0.40| 58|\n",
      "|0.35| 62|\n",
      "|0.60| 45|\n",
      "|0.37| 47|\n",
      "|0.59| 60|\n",
      "|0.72| 57|\n",
      "|0.54| 44|\n",
      "|0.40| 47|\n",
      "|0.68| 66|\n",
      "|0.66| 65|\n",
      "|0.55| 66|\n",
      "|0.60| 42|\n",
      "|0.59| 63|\n",
      "+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id AS sid, session_count AS sc FROM utilization\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+---------+-------------+\n",
      "|     event_datatime|server_|cpu_utilization|server_id|session_count|\n",
      "+-------------------+-------+---------------+---------+-------------+\n",
      "|03/05/2019 08:51:14|    100|           0.51|     0.60|           45|\n",
      "|03/05/2019 09:36:14|    100|           0.42|     0.60|           42|\n",
      "|03/05/2019 09:46:14|    100|           0.29|     0.60|           44|\n",
      "|03/05/2019 11:31:14|    100|           0.66|     0.60|           56|\n",
      "|03/05/2019 12:26:14|    100|           0.67|     0.60|           66|\n",
      "|03/05/2019 17:41:14|    100|           0.49|     0.60|           45|\n",
      "|03/05/2019 18:21:14|    100|           0.64|     0.60|           67|\n",
      "|03/05/2019 19:21:14|    100|           0.53|     0.60|           57|\n",
      "|03/05/2019 21:11:14|    100|           0.31|     0.60|           62|\n",
      "|03/05/2019 21:41:14|    100|           0.53|     0.60|           42|\n",
      "|03/06/2019 00:06:14|    100|           0.29|     0.60|           63|\n",
      "|03/06/2019 02:41:14|    100|           0.62|     0.60|           63|\n",
      "|03/06/2019 05:01:14|    100|           0.53|     0.60|           48|\n",
      "|03/06/2019 06:21:14|    100|           0.44|     0.60|           40|\n",
      "|03/06/2019 10:36:14|    100|           0.61|     0.60|           67|\n",
      "|03/06/2019 11:51:14|    100|           0.50|     0.60|           49|\n",
      "|03/06/2019 17:46:14|    100|           0.61|     0.60|           60|\n",
      "|03/06/2019 18:41:14|    100|           0.33|     0.60|           49|\n",
      "|03/07/2019 02:21:14|    100|           0.48|     0.60|           70|\n",
      "|03/07/2019 11:51:14|    100|           0.55|     0.60|           68|\n",
      "+-------------------+-------+---------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization WHERE server_id = 0.6\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|     0.35|           71|\n",
      "|     0.42|           71|\n",
      "|     0.36|           71|\n",
      "|     0.66|           71|\n",
      "|     0.58|           72|\n",
      "|     0.52|           72|\n",
      "|     0.47|           71|\n",
      "|     0.55|           71|\n",
      "|     0.35|           71|\n",
      "|     0.66|           72|\n",
      "|     0.38|           71|\n",
      "|     0.63|           72|\n",
      "|     0.72|           71|\n",
      "|     0.64|           71|\n",
      "|     0.57|           72|\n",
      "|     0.63|           71|\n",
      "|     0.40|           72|\n",
      "|     0.69|           72|\n",
      "|     0.53|           71|\n",
      "|     0.46|           71|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization WHERE session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239659"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|     0.60|           71|\n",
      "|     0.60|           71|\n",
      "|     0.60|           71|\n",
      "|     0.60|           72|\n",
      "|     0.60|           72|\n",
      "|     0.60|           71|\n",
      "|     0.60|           72|\n",
      "|     0.60|           71|\n",
      "|     0.60|           71|\n",
      "|     0.60|           74|\n",
      "|     0.60|           74|\n",
      "|     0.60|           71|\n",
      "|     0.60|           72|\n",
      "|     0.60|           74|\n",
      "|     0.60|           72|\n",
      "|     0.60|           71|\n",
      "|     0.60|           72|\n",
      "|     0.60|           71|\n",
      "|     0.60|           73|\n",
      "|     0.60|           74|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization WHERE server_id = 0.6 and session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           83|\n",
      "|     0.60|           82|\n",
      "|     0.60|           82|\n",
      "|     0.60|           82|\n",
      "|     0.60|           82|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count FROM utilization \\\n",
    "                    WHERE server_id = 0.6 and session_count > 70\\\n",
    "                    ORDER BY session_count DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third lesson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql(\"SELECT count(*) FROM utilization\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT count(*)\\\n",
    "                   FROM utilization\\\n",
    "                   WHERE session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|Quantity|\n",
      "+---------+--------+\n",
      "|     0.55|    1558|\n",
      "|     0.07|    2302|\n",
      "|     0.59|    1214|\n",
      "|     0.32|    5981|\n",
      "|     0.11|    3576|\n",
      "|     0.03|     655|\n",
      "|     0.60|    1038|\n",
      "|     0.31|    6004|\n",
      "|     0.73|       4|\n",
      "|     0.14|    4310|\n",
      "|     0.74|       1|\n",
      "|     0.57|    1423|\n",
      "|     0.25|    5519|\n",
      "|     0.36|    6089|\n",
      "|     0.68|     133|\n",
      "|     0.30|    5841|\n",
      "|     0.06|    2001|\n",
      "|     0.72|      16|\n",
      "|     0.21|    4907|\n",
      "|     0.15|    4311|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, count(*) AS Quantity\\\n",
    "                   FROM utilization\\\n",
    "                   WHERE session_count > 70 \\\n",
    "                   GROUP BY server_id\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|Quantity|\n",
      "+---------+--------+\n",
      "|     0.39|    6154|\n",
      "|     0.36|    6089|\n",
      "|     0.31|    6004|\n",
      "|     0.35|    6003|\n",
      "|     0.33|    6001|\n",
      "|     0.34|    5999|\n",
      "|     0.37|    5997|\n",
      "|     0.32|    5981|\n",
      "|     0.40|    5927|\n",
      "|     0.29|    5887|\n",
      "|     0.41|    5871|\n",
      "|     0.28|    5869|\n",
      "|     0.30|    5841|\n",
      "|     0.26|    5838|\n",
      "|     0.38|    5825|\n",
      "|     0.27|    5726|\n",
      "|     0.25|    5519|\n",
      "|     0.42|    5414|\n",
      "|     0.43|    5323|\n",
      "|     0.24|    5262|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, count(*) AS Quantity\\\n",
    "                   FROM utilization\\\n",
    "                   WHERE session_count > 70 \\\n",
    "                   GROUP BY server_id\\\n",
    "                   ORDER BY Quantity DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-----------------+\n",
      "|server_id|Minimum|Maximum|          Average|\n",
      "+---------+-------+-------+-----------------+\n",
      "|     0.39|    100|     99|81.93532661683457|\n",
      "|     0.36|    100|     99|82.23370011496141|\n",
      "|     0.31|    100|     99| 82.0886075949367|\n",
      "|     0.35|    100|     99|82.36265200732967|\n",
      "|     0.33|    100|     99|82.25012497917014|\n",
      "|     0.34|    100|     99|81.98999833305551|\n",
      "|     0.37|    100|     99|82.05035851258963|\n",
      "|     0.32|    100|     99|82.37836482193613|\n",
      "|     0.40|    100|     99|82.23097688543952|\n",
      "|     0.29|    100|     99|82.59436045524036|\n",
      "|     0.41|    100|     99|82.03236245954693|\n",
      "|     0.28|    100|     99|82.47980916680865|\n",
      "|     0.30|    100|     99| 82.3622667351481|\n",
      "|     0.26|    100|     99|82.82973621103118|\n",
      "|     0.38|    100|     99|82.26111587982832|\n",
      "|     0.27|    100|     99|82.74135522179532|\n",
      "|     0.25|    100|     99|82.67820257292988|\n",
      "|     0.42|    100|     99|81.63003324713705|\n",
      "|     0.43|    100|     99|81.54969002442232|\n",
      "|     0.24|    100|     99|83.05169137210186|\n",
      "+---------+-------+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, min(session_count) AS Minimum, max(session_count) AS Maximum, avg(session_count) AS Average\\\n",
    "                   FROM utilization\\\n",
    "                   WHERE session_count > 70 \\\n",
    "                   GROUP BY server_id\\\n",
    "                   ORDER BY count(*) DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "|server_id|Minimum|Maximum|Average|\n",
      "+---------+-------+-------+-------+\n",
      "|     0.39|    100|     99|  81.94|\n",
      "|     0.36|    100|     99|  82.23|\n",
      "|     0.31|    100|     99|  82.09|\n",
      "|     0.35|    100|     99|  82.36|\n",
      "|     0.33|    100|     99|  82.25|\n",
      "|     0.34|    100|     99|  81.99|\n",
      "|     0.37|    100|     99|  82.05|\n",
      "|     0.32|    100|     99|  82.38|\n",
      "|     0.40|    100|     99|  82.23|\n",
      "|     0.29|    100|     99|  82.59|\n",
      "|     0.41|    100|     99|  82.03|\n",
      "|     0.28|    100|     99|  82.48|\n",
      "|     0.30|    100|     99|  82.36|\n",
      "|     0.26|    100|     99|  82.83|\n",
      "|     0.38|    100|     99|  82.26|\n",
      "|     0.27|    100|     99|  82.74|\n",
      "|     0.25|    100|     99|  82.68|\n",
      "|     0.42|    100|     99|  81.63|\n",
      "|     0.43|    100|     99|  81.55|\n",
      "|     0.24|    100|     99|  83.05|\n",
      "+---------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, min(session_count) AS Minimum, max(session_count) AS Maximum, round(avg(session_count),2) AS Average\\\n",
    "                   FROM utilization\\\n",
    "                   WHERE session_count > 70 \\\n",
    "                   GROUP BY server_id\\\n",
    "                   ORDER BY count(*) DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_path = data_path + \"/server_name.csv\"\n",
    "df_server = spark.read.csv(csv_df_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "|      105| 105 Server|\n",
      "|      106| 106 Server|\n",
      "|      107| 107 Server|\n",
      "|      108| 108 Server|\n",
      "|      109| 109 Server|\n",
      "|      110| 110 Server|\n",
      "|      111| 111 Server|\n",
      "|      112| 112 Server|\n",
      "|      113| 113 Server|\n",
      "|      114| 114 Server|\n",
      "|      115| 115 Server|\n",
      "|      116| 116 Server|\n",
      "|      117| 117 Server|\n",
      "|      118| 118 Server|\n",
      "|      119| 119 Server|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_server.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server.createOrReplaceTempView(\"server_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|     0.00|\n",
      "|     0.01|\n",
      "|     0.02|\n",
      "|     0.03|\n",
      "|     0.04|\n",
      "|     0.05|\n",
      "|     0.06|\n",
      "|     0.07|\n",
      "|     0.08|\n",
      "|     0.09|\n",
      "|     0.10|\n",
      "|     0.11|\n",
      "|     0.12|\n",
      "|     0.13|\n",
      "|     0.14|\n",
      "|     0.15|\n",
      "|     0.16|\n",
      "|     0.17|\n",
      "|     0.18|\n",
      "|     0.19|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql(\"SELECT DISTINCT server_id FROM utilization ORDER BY server_id\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|          0.00|          0.78|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(server_id), max(server_id) FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "+---------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\n",
    "                     FROM utilization u \\\n",
    "                     INNER JOIN server_name sn \\\n",
    "                     ON sn.server_id = u.server_id\")\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth lesson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-80ab19a8a726>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-80ab19a8a726>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    conf = SparkConf().setAppName(\"Spark SQL Query Dataframes\").setMaster(local[*])\u001b[0m\n\u001b[1;37m                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"Spark SQL Query Dataframes\").setMaster(local[*])\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-43f0b6e3384d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df_dup = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\\\n\u001b[0m\u001b[0;32m      2\u001b[0m                         \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserver_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'101 Server'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_utilization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserver_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'102 Server'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_utilization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m85\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         Row(server_name='102 Server', cpu_utilization=85, session_count=82)]).toDF()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "df_dup = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\\\n",
    "                        Row(server_name='101 Server', cpu_utilization=80, session_count=90),\\\n",
    "                        Row(server_name='102 Server', cpu_utilization=85, session_count=80),\\\n",
    "                        Row(server_name='102 Server', cpu_utilization=85, session_count=82)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_datatime: string (nullable = true)\n",
      " |-- server_: string (nullable = true)\n",
      " |-- cpu_utilization: string (nullable = true)\n",
      " |-- server_id: string (nullable = true)\n",
      " |-- session_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
